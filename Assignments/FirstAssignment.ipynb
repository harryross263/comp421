{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First COMP421 Assignment\n",
    "Harry Ross -- 300292083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: some writing\n",
    "__Radial Basis Functions for regression__\n",
    "\n",
    "Before jumping into any substance, I'll begin with some definitions: \n",
    "\n",
    "* A Radial Basis Function (RBF) is a function mapping to real values, whose value at point x (i.e. $\\phi(x)$) depends on the distance from some (or all) other points known to us (i.e. in the training set). \n",
    "\n",
    "    In layman's terms, the term \"radial\" describes how the function depends on its distance from other points, and the term \"basis function\" refers to the distribution of this \"radiality\", if you will. Normally the distribution is defined as some sort of Gaussian decay.\n",
    "\n",
    "* Regression is defined as the continuous generalisation of the classification task. At a high level, it is the process of approximating a real valued output, given some input. \n",
    "    \n",
    "    For example, if we have a training set of paired $\\{x_n; y_n\\}$ where $x_n$ are the inputs, and $y_n \\in \\mathbb{R}$ are the targets, the task of regression is to approximate a function $\\phi(x)$ such that $\\phi(x_n)$ is arbitrarily close to $t_n$. \n",
    "    \n",
    "A simple example of how a RBF might be used to perform regression is as follows: Let $X$ be our training set. Suppose that we say that the value of $\\phi(x)$ relies on all $x_n \\in X$ (more complicated models may simulate KNN by limiting this dependence to only the closest K $x_n \\in X$ or KMeans by taking the mean of K clusters within $X$, but for simplicity we will say that $\\phi(x)$ is dependent on each $x_n \\in X$). \n",
    "\n",
    "We want to model the fact that each data point in our training set should influence $\\phi$ in a slightly different way, and so we need weights $W = \\langle w_1 ... 2_n \\rangle$. Again, for simplicity let our radial function be defined as euclidean distance, and our basis function be a Gaussian.\n",
    "\n",
    "Given this preamble, we can define $\\phi(x)$ as $$\\phi(x) = \\sum^N_{n = 1}w_n \\exp(-\\gamma ||x - x_n||^2)$$\n",
    "\n",
    "To \"train\" this model, the RBF, we need to determine $W$. Since there $N$ parameters ($\\mathtt{len}(W) = N$), one for each data point in our training set, it is possible to reach zero error on the training set, since we can solve the following equation: $$\\sum^N_{m = 1}w_m \\exp(-\\gamma ||x_n - x_m||^2) = y_n$$\n",
    "\n",
    "The solution to this particular example is best illustrated if we use matrix form to express our model. Note that the following representation is exactly identical to the previous, more succint, expression:\n",
    "\n",
    "$$\\left[\\begin{array}{r} \\exp(-\\gamma ||x_1 - x_1||^2) + \\dots \\exp(-\\gamma ||x_1 - x_N||^2) \\\\ \\vdots \\\\ \\exp(-\\gamma ||x_N - x_1||^2) + \\dots \\exp(-\\gamma ||x_N - x_N||^2) \\end{array}\\right] \\left[\\begin{array}{r} w_1 \\\\ \\vdots \\\\ w_N \\end{array}\\right] = \\left[\\begin{array}{r} y_1 \\\\ \\vdots \\\\ y_N \\end{array}\\right]$$\n",
    "\n",
    "Of the form $\\phi \\mathbf W = \\mathbf y$, where it can be seen that the solution is simply $\\mathbf W = \\phi^{-1} \\mathbf y$. \n",
    "\n",
    "Hopefully I've articulated in a relatively intuitive way how a simple RBF can be used for regression. Now to compare RBFs with neural networks...\n",
    "\n",
    "__RBFs vs. NN__\n",
    "\n",
    "Just as I rewrote the above RBF model in matrix form, we can also express a RBF as a network. The following diagram illustrates this nicely, side-by-side with a model of a regular neural network. Note that the diagram expresses a RBF that simulates a KMeans model - where the value of $\\phi(x)$ depends on the distance between x and averages from the training set $\\mu = \\langle \\mu_1 \\dots \\mu_j \\rangle$ rather than relying on the distance to every other data point in the training set:\n",
    "\n",
    "<img src=\"rbfs.png\">\n",
    "\n",
    "* The distinction: RBFs are trained by computing pairwise distances between data points within the training set. Neural networks on the other hand learn a mapping from inputs to outputs iteratively, and with no consideration of other examples in the training set (slightly true in the case of mini batch training...)\n",
    "\n",
    "* Implications for learning: Just as in the world of neural networks, RBFs range from the simple (as described in the preamble) to the extremely complicated (and simulations of other types of learning algorithms). In either case the learning rule, simply inverting the parameter matrix $\\phi$  is an extremely fast, simple and intuitive way to learn a mapping form inputs to outputs. General consensus from the literature is that RBFs don't get stuck in local optima (although I haven't seen a proof of this).\n",
    "\n",
    "* Performance pros and cons: We can see that by following the learning rule in a RBF, our training error will be zero (since we solved an equality with the targets). This is the most extreme case of overfitting, and so requires a large, diverse training set for the results to generalise well to the testing environment. On the other hand, techniques such as dropout and cross validation can help prevent overfitting in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: some math\n",
    "\n",
    "From the following series, we can see that the derivative of the softmax function $\\partial y_i / \\partial z_j$ if $i = j$ is $y_i(1 - y_i)$ and $y_i(0 - y_j) = -y_iy_j$ if $i \\neq j$:\n",
    "\n",
    "$$\\partial y_i / \\partial z_i = \\frac{e^{z_j}\\sum^C_{d = 1}e^{z_d} - 2e^{z_j}}{2\\sum^C_{d = 1}e^{z_d}} = \\frac{e^{z_j}}{\\sum^C_{d = 1}e^{z_d}}\\frac{\\sum^C_{d = 1}e^{z_d} - e^{z_j}}{\\sum^C_{d = 1}e^{z_d}} = \\frac{e^{z_j}}{\\sum^C_{d = 1}e^{z_d}}(1 - \\frac{e^{z_j}}{\\sum^C_{d = 1}e^{z_d}}) = y_i(1 i y_i)$$\n",
    "\n",
    "$$\\partial y_i / \\partial z_j = \\frac{\\partial \\frac{e^{z_i}}{\\Sigma_C}}{\\partial z_j} = \\frac{0 - e^{z_i}e^{z_j}}{\\Sigma_C * \\Sigma_C} = -\\frac{e^{z_i}}{\\Sigma_C} \\frac{e^{z_j}}{\\Sigma_C} = -y_i y_j$$\n",
    "\n",
    "Unfortunately I struggled to work from these results to the delta rule. I will talk to you in class so that I can ensure that I get comfortable with this for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: some coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See harry_backprop_play.ipynb, submitted alongside."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
