{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First COMP421 Assignment\n",
    "\n",
    "There are three parts to this assignment.\n",
    "\n",
    "Suggestion: just edit and add cells to this, and submit a notebook?\n",
    "\n",
    "Please submit using the usual ECS [submission system](https://apps.ecs.vuw.ac.nz/submit/COMP421)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: some writing\n",
    "\n",
    "Read about how to do regression with _Radial Basis Functions_ (RBFs).\n",
    "Discuss the relationship between RBFs and regular neural networks. For example, you might want to mention things like\n",
    "   1. the distinction\n",
    "   1. implications for learning? (simpler / not? faster/slower? local optima less/more?  \n",
    "   1. performance pros and cons (over/under fitting?)\n",
    "   1. regularisation same or different?\n",
    "   1. and so on\n",
    "\n",
    "Keep it to a page or so of text, but you're very welcome to demo the two and add a figure from that if you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: some math\n",
    "Suppose we have data consisting of a set of $N$ patterns $\\mathbf{x}$ which are $D$ elements long, each with a target class $\\in \\{1..K\\}$. We can think of this as a $N\\times D$ matrix $X$, and a $N\\times K$ matrix $T$. each row of $T$ contains zeros except from a single 1 in the column corresponding to the correct target class.\n",
    "\n",
    "Consider a feed-forward neural network. When the network is given the input vector $ \\mathbf{x}_{n} $ it generates an output vector $\\mathbf{y}_{n}$ via the softmax function.\n",
    "We can make an $N\\times K$ matrix $Y$ from these output vectors (one per row), and thus the network maps $X \\rightarrow Y$ and $Y$ has the same dimensions as $T$. The softmax function is\n",
    "$$ Y_{n,i} = \\frac{\\exp(\\phi_{n,i})}{Z_n}  \n",
    "$$\n",
    "where $\\phi_{n,i} = \\sum_j W_{i,j} X_{n,j}$ and $Z_n = \\sum_k \\exp(\\phi_{n,k})$.\n",
    "\n",
    "In lecture we met the \"cross entropy\" loss function:\n",
    "$$ E = \\sum_n \\sum_k T_{n,k} \\log Y_{n,k} $$\n",
    "\n",
    "(Note that sometimes it's more convenient to write this as\n",
    "$$ E = \\sum_n \\log Y_{n, c_n} $$\n",
    "where $ c_n $ is the _index of the target class_ for the $ n^\\text{th} $ item in the training set.)\n",
    "\n",
    "We motivated the cross-entropy loss by arguing that it is the _log likelihood_, ie. the probability that a stochastic form of this network would generate precisely the training classes, namely to use the softmax outputs as a _categorical distribution_ and sample classes from it.\n",
    "\n",
    "(See  [this](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/)\n",
    "which discusses cross-entropy _vs_ training error _vs_ sum-of-squared errors without refering to a stochastic model).\n",
    "\n",
    "Q: Consider a simple neural network with no hidden layers and a softmax as the output layer. Show mathematically that gradient descent of the cross entropy loss in leads to the \"delta rule\" for the weight change: $\\Delta W_{ij} \\propto \\sum_n (T_{n,i} - Y_{n,i}) X_{n,j}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: some coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore <pre>comp421/notebooks/marcus_backprop_play.ipynb</pre>  further, by pulling it from the repository and altering the code.\n",
    "Rename your local version YOURNAME_backprop.ipynb and submit it along with your assignment (e.g. your submission might be two notebooks).\n",
    "\n",
    "Currently the code simply _invents_ a random mapping to learn, and then learns it.\n",
    "You've met the sklearn \"digits\" data (you tried knn out on it).\n",
    "Adapt the Backprop code so that it can be trained on the digits data (roughly 1800 hand-written digits, each being an 8-by-8 image).\n",
    "\n",
    "I suggest doing this after doing the maths exercise above, as it should help.\n",
    "\n",
    "Currently the code uses a single transfer function throughout (either sigmoid/logistic or ReLU): it should really use one transfer function (either sigmoid/logistic or ReLU) for all the hidden units but use softmax from the (linear) outputs. When doing a backward pass, it should backpropagate errors successfully though the softmax and the preceding nonlinearities successfully -- \"checkgrad\" should confirm this when you've got it right. \n",
    "\n",
    "Once it's working, try learning the digits classifier using sigmoids versus ReLUs for the hidden units. Comment on which seems to be better, and how these results compare to the [kNN classifier we tried out in class](https://github.com/garibaldu/comp421/blob/master/notebooks/knn_on_little_digits.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
