This week is about learning the parameters of a predictor. We'll look at three flavours: polynomials, neural nets, and radial basis functions. We will see that each has its own strengths and weaknesses.

***
# fitting polynomials to data
< 1st lecture (presented by James) >

Today's lecture is a tour of the [first section]() of Chris Bishop's fantastic book [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/). The whole book is well worth getting if you plan to go further with machine learning - see the library for the hardcopy.

 
***

# Back-propagation in neural networks
< 2nd lecture (presented by Jack) >

See also [link he suggested](whatwasitagain?)


***

A basic hands-on session : if you have a laptop, bring it.

Aims:

 * get ipython/jupyter notebook going, and scikit-learn, and numpy, and matplotlib - the simplest way is via [anaconda](https://www.continuum.io/downloads) which (we think) has everything in place.
 * if you get that far, but not everyone has yet please help someone else!
 * interact with the [comp421 repository on github](https://github.com/garibaldu/comp421) - even if you never push back, you should pull it regularly.
 * if you get that far, but not everyone has yet please help someone else!
 * from the [notebooks](https://github.com/garibaldu/comp421/tree/master/notebooks) directory invoke _ipython notebook_ and step through the notebooks on "knn on little digits", and "marcus play backprop"
