{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# horsing around with the backprop algorithm\n",
    "Marcus started this see how quickly he could get backprop to stand up.\n",
    "\n",
    "Note the use of \"checkgrad\", which exhaustively confirms that the gradient calculation is in fact correct - not something to run all the time but a useful check to have.\n",
    "\n",
    "Issues:\n",
    "  * the neural net has no biases yet\n",
    "  * the learning problem is just random - better if we could read in a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import sklearn\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.cross_validation as cv\n",
    "import sklearn.neighbors as nb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision = 2, suppress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify some neuron transfer functions and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(phi):\n",
    "    \"\"\"Calculates the sigmoid of phi.\n",
    "    \n",
    "    phi - A matrix of weighted inputs i.e. np.dot(W, X).\n",
    "    \"\"\"\n",
    "    neg_phi = -1 * phi\n",
    "    \n",
    "    return (1.0 / (1.0 + np.exp(neg_phi)))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    \"\"\"Calculates the gradient of the sigmoid function at point x. Note that\n",
    "    this is hard coded to correspond directly to the sigmoid(phi) function\n",
    "    above.\n",
    "    \n",
    "    x - The value of sigmoid(phi) for some phi that we wish to know\n",
    "        the gradient of.\n",
    "    \"\"\"\n",
    "    return (x * (1 - x))\n",
    "    \n",
    "def relu(phi):\n",
    "    \"\"\"Rectified linear transfer function (ReLU).\n",
    "    \n",
    "    phi - A matrix of weighted inputs i.e. np.dot(W, X).\n",
    "    \"\"\"\n",
    "    return phi * (phi > 0.0)\n",
    "\n",
    "def grad_relu(x):\n",
    "    \"\"\"Calculates the gradient of the relu function at point x.\n",
    "    \n",
    "    x - The value of relu(phi) for some phi that we wish to know\n",
    "    the gradient of.\"\"\"\n",
    "    return 1.0 * (x > 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - implement a softmax transfer function.\n",
    "def softmax(phi):\n",
    "    return np.exp(phi) / np.sum(np.exp(phi)) \n",
    "\n",
    "def grad_softmax(x):\n",
    "    # TODO.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797, 1)\n",
      "0.0 16.0\n"
     ]
    }
   ],
   "source": [
    "# Load the sklearn digits data set.\n",
    "digits = ds.load_digits()\n",
    "inputX = digits.data\n",
    "targ = digits.target\n",
    "\n",
    "# Note that targ is currently a vector, but we want a n x 1 matrix,\n",
    "# and so we reshape...\n",
    "targ = np.reshape(targ, (len(targ), 1))\n",
    "\n",
    "print(inputX.shape, targ.shape)\n",
    "print(inputX.min(), inputX.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function we're climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_goodness(Y, targets):\n",
    "    \"\"\"Often this is called the \"Loss\" or the \"Cost function\" (and \n",
    "    given a minus sign accordingly).\n",
    "    \n",
    "    Y -\n",
    "    targets -\n",
    "    \n",
    "    returns good_vec.sum() -\n",
    "            good_vec -\n",
    "            dgood -\n",
    "    \"\"\"\n",
    "    error = targets - Y\n",
    "    \n",
    "    # Inverted parabola centered on the target outputs.\n",
    "    good_vec = -0.5 * np.power(error, 2.0)\n",
    "    \n",
    "    # dGood_vec is the (direction? delta?) of something - if output\n",
    "    # is too low, it will be positive.\n",
    "    dgood = error \n",
    "    \n",
    "    return good_vec.sum(), good_vec, dgood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the network's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are this many neurons in each layer:  [64, 5, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# We have n examples from our inputX data. Each example has inputX.shape[1]\n",
    "# dimensions, and so we need inputX.shape[1] neurons in our input layer.\n",
    "# There are targ.shape[1] different output dimensions, and so we need\n",
    "# targ.shape[1] neurons in our output layer.\n",
    "input_dimensions = inputX.shape[1]\n",
    "output_dimensions = targ.shape[1]\n",
    "\n",
    "architecture = [input_dimensions, 5, 3, output_dimensions]\n",
    "\n",
    "print(\"There are this many neurons in each layer: \", architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 activations have shape (1797, 64)\n",
      "layer 1 activations have shape (1797, 5)\n",
      "layer 2 activations have shape (1797, 3)\n",
      "layer 3 activations have shape (1797, 1)\n"
     ]
    }
   ],
   "source": [
    "# X is going to be a list giving the activations of successive layers. \n",
    "# Each element in X is a matrix, whose columns are the neurons in the\n",
    "# layer corresponding to the elements index within X. Each row in the\n",
    "# matrix corresponds to a training item, so all the matrices in X will\n",
    "# have the same number of rows.\n",
    "X = [inputX]\n",
    "n = inputX.shape[0] # The number of examples that we have.\n",
    "\n",
    "for L in range(1, len(architecture)):\n",
    "    X.append(np.zeros(shape=(n, architecture[L]), dtype=float))\n",
    "\n",
    "for L in range(len(architecture)): \n",
    "    print(\"layer\", L, \"activations have shape\", X[L].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 weights have shape ()\n",
      "layer 1 weights have shape (5, 64)\n",
      "layer 2 weights have shape (3, 5)\n",
      "layer 3 weights have shape (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Let's index weight layer by the layer they're *going* towards.\n",
    "\n",
    "# Initialise the zero'th weight layers. W is a list of matrices. The element\n",
    "# at index i has the same number of rows as the number of columns in X[i]. i.e.\n",
    "# there is one row for each neuron in the layer that the weights are *going*\n",
    "# towards. Similarly, there is a column for each neuron in the layer that the\n",
    "# weights are *coming* from.\n",
    "W  = [np.array(None)]\n",
    "dW = [np.array(None)]\n",
    "\n",
    "init_weights_scale = 0.1  # 1 / np.sqrt((X[L].shape()).max())\n",
    "\n",
    "for layer in range(1, len(X)):\n",
    "    # There is a weight from each neuron in the previous layer to each neuron\n",
    "    # in the current layer.\n",
    "    in_dimension = X[layer - 1].shape[1]\n",
    "    out_dimension = X[layer].shape[1]\n",
    "    \n",
    "    W.append(init_weights_scale * rng.normal(0, 1, size=(out_dimension, in_dimension)))\n",
    "    \n",
    "    # The change in weights is initially zero, although we want to maintain\n",
    "    # the dimensionality of the weights matrix that we just constructed.\n",
    "    dW.append(0.0 * np.copy(W[layer]))\n",
    "\n",
    "for L in range(len(W)):\n",
    "    print(\"layer\", L, \"weights have shape\", W[L].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_pass(X, W):\n",
    "    \"\"\"Takes the inputs to each layer, evaluates them with the \n",
    "    corresponding weights and then calculates the activation of\n",
    "    each neuron using the sigmoid function. No learning is done\n",
    "    at this stage, only evaluation of parameters.\n",
    "    \n",
    "    X - A list of matrices describing the inputs to each layer. The\n",
    "        inputs to the first layer (the input layer) is X[0]. In\n",
    "        general, the input to layer i is X[i - 1].\n",
    "    W - List of matrices describing the weights between each layer.\n",
    "        The weights *going* to layer i are indexed by W[i].\n",
    "        \n",
    "    returns X - the input X with new activations.\n",
    "    \"\"\"\n",
    "    for layer in range(1, len(X)):\n",
    "        # Grab the inputs to this layer, and transpose so that we\n",
    "        # can take the dot product of them with the corresponding\n",
    "        # weights.\n",
    "        inputs = X[layer - 1].transpose()\n",
    "        weighted_inputs = np.dot(W[layer], inputs).transpose()\n",
    "        \n",
    "        # The activations (outputs) of this layer are defined by the\n",
    "        # transfer function.\n",
    "        X[layer] = sigmoid(weighted_inputs)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_pass(X, W, dW, targets):\n",
    "    \"\"\"Evaluates the error of the model, based on the calc_goodness\n",
    "    function and determines the direction and the magnitude of the\n",
    "    weight change needed to optimise.\n",
    "    \n",
    "    X - A list of matrices describing the inputs to each layer. The\n",
    "        inputs to the first layer (the input layer) is X[0]. In\n",
    "        general, the input to layer i is X[i - 1].\n",
    "    W - List of matrices describing the weights between each layer.\n",
    "        The weights *going* to layer i are indexed by W[i].\n",
    "    dW - A list of matrices containing the deltas to each weight layer.\n",
    "    targets - An n x 1 matrix containing the target class for each\n",
    "              example.\n",
    "    \n",
    "    returns dW -\n",
    "    \"\"\"\n",
    "    good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
    "    epsilon = dgood\n",
    "    npats = X[0].shape[0]\n",
    "    \n",
    "    for layer in range(len(X) - 1, 0, -1):\n",
    "        psi = epsilon * grad_sigmoid(X[layer]) # Element-wise multiply.\n",
    "        n1 = X[layer - 1].shape[1]\n",
    "        n2 = psi.shape[1]\n",
    "        A = np.tile(X[layer - 1], n2).reshape(npats, n2, n1)\n",
    "        B = np.repeat(psi, n1).reshape(npats, n2, n1)\n",
    "        dW[layer] = (A * B).sum(0) # Outer product multiply.\n",
    "        epsilon = np.dot(psi, W[layer]) # Inner product multiply.\n",
    "        \n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = forward_pass(X, W)\n",
    "dW = backward_pass(X, W, dW, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkgrad(dW, X, W, targets):\n",
    "    \"\"\"Calculates the gradient directly, via pertubations to every weight.\n",
    "    This is completely daft in practical terms, but is very useful for debugging\n",
    "    as it tells us whether the backprop of errors is returning the true gradient.\n",
    "    \n",
    "    dW -\n",
    "    X -\n",
    "    W -\n",
    "    targets -\n",
    "    \n",
    "    returns None\n",
    "    \"\"\"\n",
    "    epsilon = 0.0001\n",
    "    \n",
    "    dW_test = [np.array(None)]\n",
    "    for L in range(1,len(W)):\n",
    "        dW_test.append(0.0*np.copy(W[L]))\n",
    "    \n",
    "    X = forward_pass(X,W)\n",
    "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
    "    \n",
    "    for layer in range(1, len(X)):\n",
    "        # For each destination node.\n",
    "        for j in range(W[layer].shape[0]):\n",
    "            # For each origin node.\n",
    "            for i in range(W[layer].shape[1]):\n",
    "                # Perturb the weight from (layer - 1, i) -> (layer, j).\n",
    "                (W[layer])[j, i] += epsilon\n",
    "                # Compute and store the empirical gradient estimate.\n",
    "                X = forward_pass(X, W)\n",
    "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
    "                (dW_test[layer])[j, i] = (tmp_good - base_good) / epsilon                \n",
    "                # Unperturb the weight.\n",
    "                (W[layer])[j, i] -= epsilon\n",
    "                \n",
    "    # Print the results for analysis.\n",
    "    for L in range(1,len(X)):\n",
    "        print ('-------------- layer %d --------------' %(L))\n",
    "        print ('calculated gradients:')\n",
    "        print (dW[L])\n",
    "        print ('empirical gradients:')\n",
    "        print (dW_test[L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkgrad(dW, X, W, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yay.\n",
    "The gradient seems to be right for the full MLP, so that's... progress!\n",
    "\n",
    "Let's try learning the problem then...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn(X, W, dW, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
    "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
    "    times, vals = [], []\n",
    "    next_time = 0\n",
    "    \n",
    "    prev_change = [np.array(None)]\n",
    "    for L in range(1,len(X)):\n",
    "        prev_change.append(0.0 * np.copy(W[L]))\n",
    "    \n",
    "    # now for the learning iterations\n",
    "    for step in range(num_steps):\n",
    "        X = forward_pass(X,W)\n",
    "        \n",
    "        # this is just record-keeping.......\n",
    "        if step == next_time:\n",
    "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
    "            vals.append(good_sum)\n",
    "            times.append(step)\n",
    "            next_time = step + 10\n",
    "\n",
    "        dW = backward_pass(X, W, dW, targets)\n",
    "        for L in range(1,len(X)):\n",
    "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
    "            W[L] = W[L] + change\n",
    "            prev_change[L] = change\n",
    "\n",
    "\n",
    "    return W, times, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, vals, times = learn(X, W, dW, targ, learning_rate=0.01, momentum=0.5, num_steps=10000)\n",
    "plt.plot(vals, times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
